{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Data Pre-Processing Notebook",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00000-2d32b4fd-3567-4c82-aa7d-f102425e04f7",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "In this notebook we go through most of the code used to fetch/generate the data used our project. <br> \nIn total we combine 4 data sources: \n* List of top 1000 artists from Acclaimed Music\n* Wikipedia Articles of each artist\n* Spotify API for artist to song relation mapping, metadata and audio features\n* Spotify & Genius API for lyrics",
   "metadata": {
    "tags": [],
    "cell_id": "00001-80727467-6705-4419-bd60-67cf8db0bca3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-79ee3fc8-c12c-4468-abba-c6dd0ea2c799",
    "deepnote_cell_type": "code"
   },
   "source": "# Imports\nimport os\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nfrom fa2 import ForceAtlas2\nimport community\nfrom operator import itemgetter\nimport nltk\nfrom nltk.corpus import PlaintextCorpusReader\nfrom nltk.probability import FreqDist\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom nltk.tokenize import sent_tokenize\nimport math\nfrom collections import Counter\nimport copy\nimport codecs\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import quote\nimport wikipedia\nimport urllib.request\nimport json\nimport requests\nfrom dataclasses import dataclass\nfrom typing import List\nimport re",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Getting Top 1000 artist list",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00001-4e8e3fdf-f5ba-4b3e-af2d-059e2b6280e8",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We use basic HTML parsing through `BeautifulSoup` to retrieve all the artist names",
   "metadata": {
    "tags": [],
    "cell_id": "00004-0562b08b-721d-4c01-95a5-165001e146c3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": " links = [f\"http://www.acclaimedmusic.net/061024/1948-09art{n}.htm\" for n in [\"\", \"2\", \"3\", \"4\", \"5\"]]\ndocs = [requests.get(link).text for link in links]",
   "metadata": {
    "tags": [],
    "cell_id": "00002-54c30e12-c0e5-4dc2-a49c-6368263ac951",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "artists = []\nfor doc in docs:\n    soup = BeautifulSoup(doc, 'html.parser')\n    rows = soup.find(\"table\").find_all(\"tr\")\n\n    for row in rows:\n        cols = row.find_all(\"td\")\n        if len(cols) == 11:\n            artist_name = cols[1].find(\"a\").string\n            artists.append(artist_name)",
   "metadata": {
    "tags": [],
    "cell_id": "00003-c34c9596-c13a-448e-940d-82624bb71c9d",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "When all artists have been fetched they are saved in a simple list file",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00008-7a1f0683-91a5-48e7-993e-ed5c274f71d8",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "with codecs.open(\"./artists_1000.txt\", \"w\",\"utf-8\") as f:\n    f.write(\"\\n\".join(artists))",
   "metadata": {
    "tags": [],
    "cell_id": "00004-b40f9bd1-0429-4105-ad9b-64248102ff58",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Getting artist links from Wikipedia Articles",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00002-94209b0d-7a2e-4d47-a2c6-c8851534037a",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Downloading Wikipedia \"Content\" and \"Links\"",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00004-80e99d9b-d1a0-4f4b-9a63-5514eb88287e",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "First off we use the Wikipedia API (MediaWiki API) to obtain get the article's page ID. From the page ID we can use the `Wikipedia` Python library to obtain the article content and the links in the articles. The search functionality in the library does not seem to match the Wikipedia API, which is why this is needed ",
   "metadata": {
    "tags": [],
    "cell_id": "00011-57a14003-ea45-4eaf-acda-0117289b5b2f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "def get_wikipedia_key(band):\n\n    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n    action = \"action=query\"\n    content = \"prop=revisions&rvprop=content&rvslots=*\"\n    dataformat =\"format=json\"\n    band = band.replace(' ', '_')\n    band = quote(band)\n    title = \"titles=\" + band\n\n    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n\n    # Now we get the data\n    wikiresponse = urllib.request.urlopen(query)\n    wikidata = wikiresponse.read()\n    wikitext = wikidata.decode('utf-8')\n    wikijson = json.loads(wikitext)\n\n    return list(wikijson['query']['pages'].keys())[0]\n\n# Find key and then article\ndef get_document(artist):\n    key = get_wikipedia_key(artist)\n    doc = wikipedia.page(pageid=key)\n    return doc",
   "metadata": {
    "tags": [],
    "cell_id": "00002-b3b99c55-a7cf-4b2c-ba2f-67804726201a",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "We load the artist names and then get each of their documents. When searching for the artist on Wikipedia we need to deal with ambiguity, so we try different variants of the artists name (e.g. (band) as suffix in the search term). When we find the artist we save the content of the article, (which ended up not being used in the project) and the links (if they exist in our artist list). ",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00014-d47a459c-4d03-4dc5-8c19-dcd15652ef8e",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "## Importing artist list:\nwith open(\"artists_1000.txt\") as f:\n    df_artists = f.read().split('\\n')\n\noffset = 524\nfor i, artist in enumerate(df_artists[offset:]):\n    print(f\"[{i+offset}] Artist: {artist}\")\n\n    variants = [artist, f\"{artist} (band)\", f\"{artist} (musician)\", f\"{artist} (singer)\", f\"{artist} (group)\", artist.replace(\"The\", \"the\")]\n\n    for name in variants:\n        try:\n            doc = get_document(name)\n        except:\n            continue # Try next variant\n        break # If no errors, no need to check others, we have the needed document\n\n    links = doc.links\n    content = doc.content\n    links = [link for link in links if link in df_artists and content.find(link) > 0]\n\n    artist_file_name = artist.replace(\"/\", \"-\")\n    with codecs.open(f\"./content/{artist_file_name}.txt\", \"w\",\"utf-8\") as f:\n        f.write(content)\n\n    with codecs.open(f\"./links/{artist_file_name}.txt\", \"w\",\"utf-8\") as f:\n        f.write(\"\\n\".join(links))\n    \n     ",
   "metadata": {
    "tags": [],
    "cell_id": "00003-3100e8d5-a673-4170-bf44-e300d963f2a7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Construct graph from \"Links\"",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00006-afe19074-3ed4-4d2e-9da5-fd642420323a",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "From the links we can now construct the graph. We load all the link files and create a dictionary. From this dictionary we can then build the graph in `networkx`. ",
   "metadata": {
    "tags": [],
    "cell_id": "00016-ef68b865-0822-4de6-b732-063e1652b7f4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "# Deal with different naming on Wiki versus artist text file\ntranslator = {\n    \"Public Image Ltd.\" : \"Public Image Ltd\",\n    \"Run-D.M.C.\" : \"Run-DMC\",\n    \"Notorious B.I.G.\" : \"The Notorious B.I.G.\",\n    \"The Bee Gees\": \"Bee Gees\",\n    \"N.W.A.\" : \"N.W.A\"\n}\n\ndef translate(artist):\n    if artist in translator:\n        return translator[artist]\n    else:\n        return artist",
   "metadata": {
    "tags": [],
    "cell_id": "00007-bb5f3c98-cc42-4de9-b94d-9a11003d01f5",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Create dictionary with links by artist\nartist_links = dict()\n\nfor artist in artists:\n    artist_file_name = artist.replace(\"/\", \"-\")\n    with open(f\"./links/{artist_file_name}.txt\", \"r\") as f:\n        links = f.read().split(\"\\n\")\n        links = [link for link in links if link != \"\"]\n        links = [translate(link) for link in links]\n        artist_links[artist] = links",
   "metadata": {
    "tags": [],
    "cell_id": "00008-b5ace8ec-568f-405c-87ba-325340dbfb39",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Add links to graph\nG = nx.DiGraph()\nG.add_nodes_from(artists)\nfor artist in artists: #add links\n    edges = [(artist, to_artist) for to_artist in artist_links[artist]]\n    G.add_edges_from(edges)",
   "metadata": {
    "tags": [],
    "cell_id": "00009-9f9131d9-72ec-48e9-b152-5070c9b90cb3",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "nx.draw(G, with_labels = False, node_size = 20) # takes 2 minutes",
   "metadata": {
    "tags": [],
    "cell_id": "00011-48ef2044-0cd8-47bb-8c58-6eefe1faa8b7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "From the graph we can now obtain the largest connected component (GCC), which will be used for the rest of the analysis. The artists contained in the GCC are saved in a text file and the whole Python object is saved with pickle.",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00022-0f24eb6e-d7af-44d6-a1ee-82611d835d75",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "## Extracting the GCC (giant connected component):\nG_undirected = G.to_undirected()\nlargest_cc = max(nx.connected_components(G_undirected), key=len)\nGCC = G.subgraph(largest_cc).copy()",
   "metadata": {
    "tags": [],
    "cell_id": "00010-3db0fbda-c21e-49f9-b13b-739a2e487b85",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Save list of artist that we need\ntextfile = open(\"GCC_artist.txt\",\"w\")\nfor element in list(largest_cc):\n    textfile.write(element + \"\\n\")\ntextfile.close()",
   "metadata": {
    "tags": [],
    "cell_id": "00012-4b72f3a9-239c-4656-80ec-9c7d88142b0b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Save graph\nwith open(\"GCC_Grahp.pickle\", 'wb') as f:\n    pickle.dump(GCC, f)",
   "metadata": {
    "tags": [],
    "cell_id": "00013-302bc80c-521b-45ef-9497-6cd3f08ac676",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Getting Spotify Data",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00004-3e19c8aa-d0cb-4a82-b682-a01889cdb020",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Spotify has for many years had a nicely documented API that can be used. All we need to do is register an \"app\" to our personal Spotify account. We then need to generate an access token for the API. For convenience this can be generated temporally from the API Console (https://developer.spotify.com/console/). This expires after an hour. <br><br>\nIn this step we use 3 endpoints in the Spotify API:\n- /search\n- /artists\n- /audio-features\n\nWe do a text search using the /search endpoint. The term is the artist name given from our Top 1000 list. This gives us a collection of artists, from which we choose the first one (in the hope that Spotify search engine makes the best guess). <br>\nFrom the found artist we get the artist ID. This is the used with the /artists endpoint to get the top tracks (using a sub-endpoint). From the returned collection we again choose the first item, as it is sorted by popularity. This item gives us the track name, track ID and other metadata like release date. <br>\nFrom the track ID we can look up audio features. These features are created by Spotify and gives a value for different properties like danceability, acousticness and so on. <br><br>\nCodewise, we create some dataclasses that help us put names on our properties. Otherwise it's just a simple REST JSON-based API.",
   "metadata": {
    "tags": [],
    "cell_id": "00027-fdf11d80-848c-4340-b23a-eb9e07b9f8ef",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "# Expired\nSPOTIFY_ACCESS_TOKEN = 'BQAXSBwgJRPFlXp_wwjjWvAZVyKNhNKaYZ9yalrETtLLKkJXa4XFhPqbWvzDbAG_QSrqMwmCLK-8LeciOkmykFqiFSsjyCM2ohEmkziw3Yu4SRl-MiCe829WWiazPqq9Cfs2te8pLC0xMg'",
   "metadata": {
    "tags": [],
    "cell_id": "00015-20095cf9-d8a5-42d9-b51b-400e7d987729",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass Artist:\n    artist_name: str\n    artist_id: str\n    genres: List[str]\n    followers: int\n    image_url: str\n\n@dataclass\nclass Track:\n    track_name: str\n    track_id: str\n    # Audio Features\n    danceability : float\n    energy: float\n    loudness: float\n    speechiness: float\n    acousticness: float\n    instrumentalness: float\n    liveness: float\n    valence: float\n    tempo: float\n    release_date: str\n\ndef get_json(url):\n    r = requests.get(url, headers={'Authorization': 'Bearer '+ SPOTIFY_ACCESS_TOKEN})\n    return r.json()\n\n# Find artist by seaching for term\ndef search(artist_name: str):\n    artist_name_url = quote(artist_name)\n    search_base_url = 'https://api.spotify.com/v1/search'\n    limit = 3\n    item_type = 'artist'\n    url = f\"{search_base_url}?query={artist_name}&limit={limit}&type={item_type}\"\n\n    json = get_json(url)\n    try:\n        items = json[\"artists\"][\"items\"]\n        a_obj = items[0]\n    except:\n        print(json)\n    return Artist(artist_name=artist_name, artist_id=a_obj[\"id\"], genres=a_obj[\"genres\"], followers=a_obj[\"followers\"][\"total\"], image_url=a_obj[\"images\"][0][\"url\"])\n\ndef find_top_track(artist: Artist):\n    artists_base_url = 'https://api.spotify.com/v1/artists'\n    audio_feature_base_url = 'https://api.spotify.com/v1/audio-features'\n    market = \"DK\"\n    \n    # Find top trapcks by artist\n    url = f\"{artists_base_url}/{artist.artist_id}/top-tracks?market={market}\"\n    json = get_json(url)\n    id = json[\"tracks\"][0][\"id\"]\n    name = json[\"tracks\"][0][\"name\"]\n    release_date = json[\"tracks\"][0][\"album\"][\"release_date\"]\n\n    # Find audio features by track\n    url = f\"{audio_feature_base_url}/{id}\"\n    json = get_json(url)\n\n    if 'error' in json:\n        print(\"Error in \", name)\n        return Track(track_name=name, track_id=id, \n            danceability = 0,\n            energy = 0,\n            loudness = 0,\n            speechiness = 0,\n            acousticness = 0,\n            instrumentalness = 0,\n            liveness = 0,\n            valence = 0,\n            tempo = 0,\n            release_date=release_date\n        )\n    else:\n        return Track(track_name=name, track_id=id, \n            danceability = json[\"danceability\"],\n            energy = json[\"energy\"],\n            loudness = json[\"loudness\"],\n            speechiness = json[\"speechiness\"],\n            acousticness = json[\"acousticness\"],\n            instrumentalness = json[\"instrumentalness\"],\n            liveness = json[\"liveness\"],\n            valence = json[\"valence\"],\n            tempo = json[\"tempo\"],\n            release_date=release_date\n        )\n",
   "metadata": {
    "tags": [],
    "cell_id": "00015-f39c1935-0a27-4a0a-963a-0f36eb230c64",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "artists = []\nwith open(\"artists_1000.txt\") as f:\n    artists = f.read().split('\\n')",
   "metadata": {
    "tags": [],
    "cell_id": "00017-f2cd8c33-85bb-41b0-a30b-96e6342544f7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Deal with different naming on Wiki versus artist text file\ndef translate(artist):\n    translator = {\n        \"Ian Dury and The Blockheads\" : \"The Blockheads\",\n        \"The Dead Kennedys\" : \"Dead Kennedys\",\n        \"The Mahavishnu Orchestra\" : \"Mahavishnu Orchestra\",\n        \"The Mekons\" : \"Mekons\",\n        \"Sam the Sham and The Pharaos\" : \"Sam the Sham and The Pharaohs\",\n        \"Michael Hurley/The Unholy Modal Rounders\" : \"Michael Hurley\",\n        \"Rythim Is Rythim\" : \"Rhythim Is Rhythim\",\n        \"Question Mark and the Mysterians\" : \"? & The Mysterians\",\n        \"The Screaming Trees\" : \"Screaming Trees\",\n        \"The Sparks\" : \"Sparks\",\n        \"The Raspberries\" : \"Raspberries\"\n    }\n    if artist in translator:\n        return translator[artist]\n    else:\n        return artist",
   "metadata": {
    "tags": [],
    "cell_id": "00018-8a11e999-2a7f-4976-9f5c-21b91b41cede",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "offset = 735\nfor idx, artist_name in enumerate(artists[offset:]):\n    print(f\"[{idx+offset}]: {artist_name}\")\n    translated_artist_name = translate(artist_name)\n    if translated_artist_name != artist_name:\n        print(f\" - Translated to {translated_artist_name}\")\n\n    # Search for Spotify artist by Acclaimed artist name\n    artist = search(translated_artist_name)\n\n    # Find top track for found artist\n    track = find_top_track(artist)\n    rows.append({**artist.__dict__, **track.__dict__}) # For making columns in future DataFrame\n    time.sleep(0.25) # Don't smash Spotify API to avoid potential ban",
   "metadata": {
    "tags": [],
    "cell_id": "00019-e0b50ef0-2c90-49dd-820a-8326a4ff9f75",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Create DataFrame\ndf_artists = pd.DataFrame(rows)\ndf_artists.drop_duplicates(subset=[\"artist_name\"], inplace=True)\ndf_artists = df_artists[df_artists[\"artist_name\"] != \"The Original Soundtrack\"]\ndf_artists",
   "metadata": {
    "tags": [],
    "cell_id": "00020-65b42bcb-4c7b-4ce4-a66f-28cd6fa700fa",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Getting Lyrics ",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00005-2da94ee7-aa01-4a7e-9b99-14ced74beee5",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Getting lyrics directly from Spotify",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00022-f0c1f936-f2c7-49c1-89b9-368d8a77a752",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "The lyrics feature is brand new on Spotify. That is probably why it is not available on the official API. So, inspired by a JavaScript library we were able to find, we used the Spotify Web Player in the browser, looked in the Network tab of the Developer Tools and found the request to some internal REST API. Using right-click, copy > \"Copy as fetch\" we could then see the exact request with required headers (like \"app-platform\": \"WebPlayer\"). By \"replaying\" the request from Python with the track ID replaced by the track ID we wanted to find the lyrics of, we could get lines of the lyrics, even with timestamps, although that was not needed in this project.",
   "metadata": {
    "tags": [],
    "cell_id": "00036-27b4518e-f03f-4928-8a27-4fc9a1513b42",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "df_artists = pd.read_csv(\"./spotify_data.csv\", index_col=0)",
   "metadata": {
    "tags": [],
    "cell_id": "00023-8a7e621d-6f30-4f0d-8bd1-dc4a749e1483",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Expired. Obtained by using Web Player and looking into Developer Tools in the browser.\nSPOTIFY_ACCESS_TOKEN = 'BQBuDVO7QS10HH72STkMtxNOX4-yZr4urMfganhnFQ_W-KHub1wDAILVSIr__sRPojcolSngaISqP8eD7dlMSQZ05T3Gixg2mNdCOx1b81daPpKm9T6YrajJb6k2dTTnqhH3Zqzz49e9CS5eAd5vtkSxp7y9pJFecZMXG_fafHly5AFFVEfps3M6gZY_2VSiBzOib4Zs9GHot7Vje_ls9c8FE79FuG-5yJeXkr3zmjfOGjz6SDWcUjtX1dY3EJ3vJSjh3yRulMAIVdfsOtTxzw37cXYOiuNJOPS1Uwk'",
   "metadata": {
    "tags": [],
    "cell_id": "00024-e6d0b1da-6eff-455f-84ee-356636ca1c15",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def get_lyrics(track_id):\n    url = f\"https://spclient.wg.spotify.com/color-lyrics/v2/track/{track_id}?format=json&vocalRemoval=false&market=from_token\"\n    print(url)\n    r = requests.get(url, headers={\n        \"accept\": \"application/json\",\n        \"accept-language\": \"da\",\n        \"app-platform\": \"WebPlayer\",\n        \"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"96\\\", \\\"Google Chrome\\\";v=\\\"96\\\"\",\n        \"sec-ch-ua-mobile\": \"?0\",\n        \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-site\": \"same-site\",\n        \"spotify-app-version\": \"1.1.73.498.gd50d3243\",\n        \"referrer\": \"https://open.spotify.com/\",\n        \"referrerPolicy\": \"strict-origin-when-cross-origin\",\n        \"authorization\": \"Bearer \"+ SPOTIFY_ACCESS_TOKEN\n    })\n    js = None\n    try:\n        js = r.json()\n    except:\n        print(\"No lyrics found\")\n    return js\n\ndef format_lyrics_to_text(lyrics):\n    lines = lyrics[\"lyrics\"][\"lines\"]\n    text = \"\\n\".join(map(lambda x: x[\"words\"],lines))\n    text = text.replace(\"â™ª\", \"\")\n    return text\n\ndef write_lyrics_text_to_file(artist_name, text):\n    artist_name = artist_name.replace(\"/\", \"-\")\n    with codecs.open(f\"./lyrics/{artist_name}.txt\", \"w\",\"utf-8\") as f:\n        f.write(text)",
   "metadata": {
    "tags": [],
    "cell_id": "00024-f0357880-2f06-4caf-ac2a-cdf2f24b46a6",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "offset = 0\nrows = [(row[\"artist_name\"], row[\"track_id\"]) for _, row in df_artists.iterrows()]\nfor idx, (artist_name, track_id) in enumerate(rows[offset:]):\n    print(f\"[{idx+offset}]: {artist_name}\")\n\n    lyrics = get_lyrics(track_id)\n    if lyrics is not None:\n        text = format_lyrics_to_text(lyrics)\n        write_lyrics_text_to_file(artist_name,text)\n    else: \n        error_artists.append(artist_name)\n    time.sleep(0.35) # Don't smash the API with requests to avoid potential IP ban",
   "metadata": {
    "tags": [],
    "cell_id": "00026-6a9b8f12-9890-4c30-af36-6a2a16482074",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Some songs on Spotify had no lyrics. For these artists/top song we saved the artist in a .CSV for further processing.",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00041-e03ba32f-bc13-4fc6-ac1a-53d4177e842a",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "print(len(error_artists))\ndf_error_artists = df_artists[df_artists[\"artist_name\"].isin(error_artists)][[\"artist_name\", \"artist_id\", \"track_name\", \"track_id\"]]\ndf_error_artists.to_csv(\"./missing_lyrics.csv\")\ndf_error_artists",
   "metadata": {
    "tags": [],
    "cell_id": "00027-0efdc266-4d00-4ab3-99a4-9d66fd49b9ec",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Getting missing lyrics not found on Spotify",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00020-42508490-2469-49b9-be85-48495645c020",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "The song for which we could not find the lyrics on Spotify, we turned to the Genius API (online lyrics service)",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00044-6eccf953-0062-4a97-bb40-c14cc18ce5eb",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "missing_lyrics= pd.read_csv('missing_lyrics.csv')",
   "metadata": {
    "tags": [],
    "cell_id": "00029-eb75f423-29ff-4a1d-ae70-7d5a21718349",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Since our track names are given by the Spotify API, and we now want to search on Genius, we need to clean the track names of text like \"... - Remastered\", \"... (feat. DJ John Doe)\" etc. This was done using the below regular expression. <br> <br>\nThe Genius API, although handy for our purpose, is not as well built at the Spotify API. This means that when there is no result it returns some list of other popular songs. However, when there is a result it returns simply the lyric. To know when we had success or not we found that we could look for text like [Verse 1], [Chorus] and so on. Again a regular expression could help us here. If there was no proper results, we simply save and empty lyric file.",
   "metadata": {
    "tags": [],
    "cell_id": "00046-ac3e688c-7bd3-49ac-a166-573efe49c6a3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "##Removing () in track\nfor i in range(len(missing_lyrics.track_name)):\n    missing_lyrics.track_name[i] = re.sub(\"[\\(\\[].*?[\\)\\]]|\\-.*\", \"\", missing_lyrics.track_name.iloc[i])",
   "metadata": {
    "tags": [],
    "cell_id": "00030-cb95655f-f02c-43ec-a5c9-a94f6aebf526",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "lyrics_all = []\nfor i in range(len(missing_lyrics)):\n    try:\n        artist = genius.search_artist(missing_lyrics.artist_name.iloc[i], max_songs=0, sort=\"title\")\n        song = artist.song(missing_lyrics.track_name.iloc[i])\n        if bool(re.match('\\[Verse .\\]|\\[Intro\\]|\\[Chorus\\]|\\[Outro\\]|\\[Bridge\\]', song.lyrics)):\n            artist_name = missing_lyrics.artist_name.iloc[i].replace(\"/\", \"-\")\n            with codecs.open(f\"./lyrics/{artist_name}.txt\", \"w\",\"utf-8\") as f:\n                f.write(song.lyrics)\n  \n        else:\n            artist_name = missing_lyrics.artist_name.iloc[i].replace(\"/\", \"-\")\n            with codecs.open(f\"./lyrics/{artist_name}.txt\", \"w\",\"utf-8\") as f:\n                f.write(\"\")\n    except:\n        artist_name = missing_lyrics.artist_name.iloc[i].replace(\"/\", \"-\")\n        with codecs.open(f\"./lyrics/{artist_name}.txt\", \"w\",\"utf-8\") as f:\n            f.write(\"\")",
   "metadata": {
    "tags": [],
    "cell_id": "00031-6df1d016-0910-461d-814d-147b6989906e",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Tokenization of lyrics",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00029-0d739239-5ecc-4d51-9610-f50d09f07d92",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "When we have fetched all the lyrics that we are able to, we need to process them into list of tokens. This is used for lexical analysis and sentiment using the vader method. We tokenize the lyrics and clean the tokens of non-alphabetic tokens, and stopwords in english, spanish, german and french. We also lemmatize the words, so we get their stem.",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00050-86833302-e5e7-4e75-ac2c-4b2c7910df68",
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "source": "nltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nsw = stopwords.words(\"english\")\nsw_spanish = stopwords.words(\"spanish\")\nsw_german = stopwords.words(\"german\")\nsw_french = stopwords.words(\"french\")\ntokenizer = WordPunctTokenizer()\nlemmatizer = WordNetLemmatizer()",
   "metadata": {
    "tags": [],
    "cell_id": "00029-0c697eae-587b-4e92-a4b2-da25bee72e9a",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "lyrics_files = os.listdir(\"./lyrics\" )\noffset = 0\nfor i, file_name in enumerate(lyrics_files[offset:]):\n    print(f\"[{i+1} / {len(lyrics_files)}]: {file_name}\")\n    with codecs.open(f\"./lyrics/{file_name}\", \"r\", \"utf-8\") as f:\n        lyrics_text = f.read()\n\n        tokens = tokenizer.tokenize(lyrics_text)\n\n        # Only include words from the alphabetic \n        tokens = [t for t in tokens if t.isalpha()]\n        tokens = [t.lower() for t in tokens]\n\n        # Remove stopwords from four different language \n        tokens = [t for t in tokens if t not in sw]\n        tokens = [t for t in tokens if t not in sw_spanish]\n        tokens = [t for t in tokens if t not in sw_german]\n        tokens = [t for t in tokens if t not in sw_french]\n\n        # Lemmatize \n        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n\n        with codecs.open(f\"./tokens/{file_name}\", \"w\", \"utf-8\") as f:\n            f.write(\"\\n\".join(tokens))",
   "metadata": {
    "tags": [],
    "cell_id": "00031-f5f1ef6c-8846-4c10-88b4-54e6287e55b7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d42bf612-b695-4354-ae90-1a8fa8a214f4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "90689a31-8ea5-4b5c-8122-386c495fd3e4",
  "deepnote_execution_queue": []
 }
}